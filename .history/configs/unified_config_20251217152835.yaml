# ============================================================================
# VLM导航系统 - 统一配置文件 (Unified Configuration)
# ============================================================================
# 本文件整合了所有系统参数，包括：
# - 环境和场景配置
# - Agent和传感器配置
# - VLM模型和API配置
# - 系统级参数和输出目录
# - 提示工程模板
# - 数据和路径配置
#
# 所有参数参考 tests/test_habitat_interactive_enhanced.py 实现
# ============================================================================

# ============================================================================
# 【部分1】场景和环境配置
# ============================================================================
environment:
  # ──────────────────────────────────────────────────────────────────────
  # 场景配置
  # ──────────────────────────────────────────────────────────────────────
  scene:
    # Habitat场景文件路径（GLB格式）
    path: './data/scene_datasets/habitat-test-scenes/3dExport1212f.glb'
    # 预编译的NavMesh文件路径（用于导航计算）
    navmesh_path: null # null = 自动计算

  # ──────────────────────────────────────────────────────────────────────
  # Agent配置（参考test_habitat_interactive_enhanced.py）
  # ──────────────────────────────────────────────────────────────────────
  agent:
    # Agent的视点高度 (单位:米) - 【参考值】0.55m
    height: 0.55
    # Agent的碰撞半径 (单位:米) - 【参考值】0.28m
    radius: 0.28

  # ──────────────────────────────────────────────────────────────────────
  # 传感器配置（RGB摄像头）
  # ──────────────────────────────────────────────────────────────────────
  sensors:
    # 前置RGB摄像头 - 第一人称视角
    front_camera:
      enabled: true
      # UUID - 必须与Habitat返回的键名一致
      uuid: 'front_rgb'
      # 分辨率 [height, width] - 【参考值】[720, 1280]
      resolution: [720, 1280]
      # 水平视场角 (度) - 【参考值】90度
      hfov: 90.0
      # 位置相对于Agent [x, y, z] - 【参考值】前方下方
      position: [0.0, 0.55, -0.6]
      # 朝向四元数 [w, x, y, z] - 【参考值】向前看
      orientation: [1.0, 0.0, 0.0, 0.0]

    # 前置深度摄像头 - 与前置RGB同位（用于VLM辅助判断距离/障碍）
    # 说明：
    # - enabled=true 时，Habitat 将返回深度观测，并在 VLM 决策时与 RGB 一起输入。
    # - 深度观测会在代码侧做可视化（灰度/近亮远暗），再发送给VLM。
    front_depth:
      enabled: true
      # UUID - 必须与Habitat返回的键名一致
      uuid: 'front_depth'
      # 分辨率 [height, width] - 默认与前置RGB一致
      resolution: [720, 1280]
      # 水平视场角 (度) - 默认与前置RGB一致
      hfov: 90.0
      # 位置相对于Agent [x, y, z] - 默认与前置RGB一致（y 会强制跟随 agent.height）
      position: [0.0, 0.55, -0.2]
      # 可选：若需要可配置朝向（欧拉角 pitch, roll, yaw），可在此填写；不填则使用默认朝向
      orientation: null

    # 后置摄像头 - 俯视图（参考test_habitat_interactive_enhanced.py）
    back_camera:
      enabled: true
      # UUID - 必须与Habitat返回的键名一致
      uuid: 'top_down_view'
      # 分辨率 [height, width] - 【参考值】[480, 640]
      resolution: [480, 640]
      # 水平视场角 (度)
      hfov: 90.0
      # 位置相对于Agent [x, y, z] - 【参考值】上方1.6米，后方1.0米
      position: [0.0, 1.6, 1.0]
      # 朝向 (欧拉角) [pitch, roll, yaw] - 【参考值】向下看
      # 说明：YAML 不支持 "-pi/5" 这类表达式，需写成数值
      orientation: [-0.6283185307, 0.0, 0.0] # -π/5 ≈ -36度

  # ──────────────────────────────────────────────────────────────────────
  # 动作配置（参考test_habitat_interactive_enhanced.py）
  # ──────────────────────────────────────────────────────────────────────
  actions:
    move_forward:
      # 单步前进距离 (米)
      amount: 0.25
    turn_left:
      # 单次左转角度 (度)
      amount: 10.0
    turn_right:
      # 单次右转角度 (度)
      amount: 10.0

  # ──────────────────────────────────────────────────────────────────────
  # 物理和渲染配置
  # ──────────────────────────────────────────────────────────────────────
  physics:
    # 是否启用物理引擎
    enabled: true
    # 物理配置文件路径
    config_file: './data/default.physics_config.json'
    # 时间步 (秒)
    timestep: 0.016
    # 重力加速度 [x, y, z]
    gravity: [0.0, -9.8, 0.0]

  # ──────────────────────────────────────────────────────────────────────
  # 灯光配置（用于对齐 enhanced viewer 的“场景默认灯光”行为）
  # ──────────────────────────────────────────────────────────────────────
  lighting:
    # 是否覆盖场景自带灯光默认值
    override_scene_light_defaults: true
    # 灯光方案 key。特殊值 'DEFAULT_LIGHTING_KEY' 会映射到 habitat_sim.gfx.DEFAULT_LIGHTING_KEY
    scene_light_setup: 'DEFAULT_LIGHTING_KEY'

  # ──────────────────────────────────────────────────────────────────────
  # NavMesh 配置（对齐 enhanced viewer 的 recompute_navmesh 参数）
  # ──────────────────────────────────────────────────────────────────────
  navmesh:
    # 计算 NavMesh 时是否包含静态物体
    include_static_objects: true
    # 初始是否显示 NavMesh 覆盖（默认关闭；可在交互式 viewer 打开）
    visualization_default: false

  # ──────────────────────────────────────────────────────────────────────
  # 渲染后处理（用于提高画面亮度，便于VLM识别）
  # ──────────────────────────────────────────────────────────────────────
  rendering:
    # 是否启用渲染后处理（关闭可保持原始观测亮度）
    enabled: true
    # 亮度增益系数（1.0=不变；>1 更亮）
    brightness_factor: 2.0

  # ──────────────────────────────────────────────────────────────────────
  # 机器人配置（可选）
  # ──────────────────────────────────────────────────────────────────────
  robot:
    # 是否加载机器狗模型
    enabled: true
    # 机器狗URDF路径
    urdf_path: './data/robots/hab_spot_arm/urdf/hab_spot_arm.urdf'
    leg_animation:
    enabled: true
    checkpoint: "data/robots/spot_data/spot_walking_trajectory.csv"
    use_range: [107, 863]  # 使用前100帧
    play_i_perframe: 1   # 每步播放1帧
      
    leg_init_params: [0.0, 0.7, -1.5, 0.0, 0.7, -1.5, 
                      0.0, 0.7, -1.5, 0.0, 0.7, -1.5]  # 12个腿部关节初始角度

    # 初始位置 [x, y, z]
    initial_position: [0.0, 0.0, 0.0]

    # 初始旋转（四元数 [w, x, y, z]）。默认值参考 enhanced viewer: 绕 Y 轴约 90°。
    initial_rotation_wxyz: [0.707, 0.0, 0.707, 0.0]
    # 初始 yaw（度）。仅当 initial_rotation_wxyz 未设置时使用。
    initial_yaw_deg: null
    # 与Agent对齐的yaw角 (度)
    yaw_align_deg: null # null = 自动计算

    # 机器狗跟随Agent时的高度偏移（米）
    height_offset: 0.6

# ============================================================================
# 【部分2】Agent系统配置
# ============================================================================
agents:
  # ──────────────────────────────────────────────────────────────────────
  # Agent1 - 动作决策智能体
  # ──────────────────────────────────────────────────────────────────────
  agent1:
    enabled: true
    # 记忆缓冲区大小（保留最近N个观测和动作）
    memory_size: 15
    # 每次决策生成的动作步数（需与 Agent1 prompt/解析一致）
    action_steps: 3
    # 是否使用VLM进行决策
    use_vlm: true

    # 长期记忆（关键事实/路线指路等）
    long_term_memory:
      enabled: true
      max_entries: 180
      max_chars_in_prompt: 1400
      persist_path: null

    # 经历/空间变化记忆（刚走过的经历，用于减少原地循环、增强空间理解）
    episodic_memory:
      enabled: true
      max_entries: 180
      max_chars_in_prompt: 1000
      persist_path: null

  # ──────────────────────────────────────────────────────────────────────
  # Agent2 - 路径规划和重规划智能体
  # ──────────────────────────────────────────────────────────────────────
  agent2:
    enabled: false
    # 记忆缓冲区大小
    memory_size: 15
    # 可通行性阈值 (0-1)
    passability_threshold: 0.6
    # 最大重规划尝试次数
    replan_max_attempts: 3

    long_term_memory:
      enabled: true
      max_entries: 80
      max_chars_in_prompt: 1000
      persist_path: null

    episodic_memory:
      enabled: true
      max_entries: 120
      max_chars_in_prompt: 800
      persist_path: null

# ============================================================================
# 【部分3】VLM模型和API配置
# ============================================================================
vlm:
  # ──────────────────────────────────────────────────────────────────────
  # 模型配置
  # ──────────────────────────────────────────────────────────────────────
  model:
    # 模型名称/路径（用于API请求）
    name: './model/Qwen3-VL-8B-Instruct'
    # 模型版本标签
    version: 'instruct'

  # ──────────────────────────────────────────────────────────────────────
  # API配置
  # ──────────────────────────────────────────────────────────────────────
  api:
    # VLM API端点 (OpenAI兼容接口)
    endpoint: 'http://localhost:8000/v1/chat/completions'
    # 请求超时时间 (秒)
    timeout: 60
    # 连接超时 (秒) - 只控制 TCP/HTTP 连接建立阶段
    connect_timeout: 10
    # 失败重试次数
    max_retries: 3

  # ──────────────────────────────────────────────────────────────────────
  # 推理参数
  # ──────────────────────────────────────────────────────────────────────
  inference:
    # 最大生成令牌数
    max_tokens: 1024
    # 采样温度 (0=确定性, 1=随机性)
    temperature: 0.7
    # Top-p采样阈值
    top_p: 0.9
    # 重复惩罚系数
    repetition_penalty: 1.1

  # ──────────────────────────────────────────────────────────────────────
  # 图像处理配置
  # ──────────────────────────────────────────────────────────────────────
  image_processing:
    # 是否对发送给远程VLM的图像做缩放/压缩（提升上传与端到端延迟）
    enabled: true
    # 前置RGB图像最大分辨率 [height, width]（单帧输入：使用“最后一次动作后的观测”）
    max_size: [384, 640]
    # 前置RGB JPEG压缩质量 (1-100)
    quality: 70
    # 楼层平面图最大分辨率（需要保证文字/结构清晰）
    floorplan_max_size: [780, 780]
    # 楼层平面图压缩质量
    floorplan_quality: 80
    # DataURL 编码缓存容量（用于平面图等重复图像）
    cache_max_entries: 32

    # 深度图可视化/压缩（仅在启用 environment.sensors.front_depth.enabled 时生效）
    depth:
      # 深度可视化最大距离（米）。超过此距离将被裁剪。
      max_depth_m: 10.0
      # 近亮远暗（更符合人类直觉，便于VLM识别）。false 表示近暗远亮。
      invert: true

# ============================================================================
# 【部分4】系统级配置
# ============================================================================
system:
  # ──────────────────────────────────────────────────────────────────────
  # 日志配置
  # ──────────────────────────────────────────────────────────────────────
  logging:
    # 日志级别: DEBUG, INFO, WARNING, ERROR, CRITICAL
    level: 'INFO'
    # 日志文件格式
    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    # 日期格式
    date_format: '%Y-%m-%d %H:%M:%S'

  # ──────────────────────────────────────────────────────────────────────
  # 输出目录配置
  # ──────────────────────────────────────────────────────────────────────
  output:
    # 输出根目录
    base_dir: './output'
    # 是否为每次运行创建独立的时间戳目录
    use_timestamp_dir: true
    # 子目录配置
    subdirs:
      frames: 'frames'
      videos: 'videos'
      logs: 'logs'
      paths: 'paths'
      metrics: 'metrics'
      vlm_inputs: 'vlm_inputs'
      vlm_outputs: 'vlm_outputs'
    # 是否保存中间帧
    save_intermediate_frames: false

  # ──────────────────────────────────────────────────────────────────────
  # 数据保存配置
  # ──────────────────────────────────────────────────────────────────────
  data_saving:
    # 是否保存VLM输入数据（JSON格式）
    save_vlm_inputs: true
    # 是否保存VLM输出数据（JSON格式）
    save_vlm_outputs: true
    # 是否保存导航指标
    save_metrics: true
    # 是否保存RGB帧（只在VLM决策时保存）
    save_rgb_frames: true
    # RGB帧图像格式 (jpeg, png)
    rgb_frame_format: 'jpeg'
    # RGB帧JPEG质量 (0-100)
    rgb_frame_quality: 80

  # ──────────────────────────────────────────────────────────────────────
  # 可视化配置
  # ──────────────────────────────────────────────────────────────────────
  visualization:
    # 是否启用实时可视化（参考run.py中的--enable-viz参数）
    enabled: false
    # 显示延迟 (毫秒)
    display_delay_ms: 1
    # 是否显示信息叠加
    show_info_overlay: true

# ============================================================================
# 【部分5】数据和路径配置
# ============================================================================
paths:
  # ──────────────────────────────────────────────────────────────────────
  # 项目路径
  # ──────────────────────────────────────────────────────────────────────
  project:
    # 项目根目录（相对路径）
    root: '.'
    # 源代码目录
    src_dir: './src'
    # 配置文件目录
    config_dir: './configs'
    # 数据目录
    data_dir: './data'
    # 测试目录
    test_dir: './tests'

  # ──────────────────────────────────────────────────────────────────────
  # 数据路径
  # ──────────────────────────────────────────────────────────────────────
  data:
    # 场景数据目录（用于工具/验证；具体场景文件路径以 environment.scene.path 为准）
    scene_data_dir: './data/scene_datasets/habitat-test-scenes'
    # 门表 Excel文件（场景导航元数据）
    door_table: './data/door_table.xlsx'
    # 组件表 Excel文件（场景组件清单）
    component_table: './data/component_table.xlsx'
    # 楼层平面图目录
    floorplan_dir: './data/map'
    # 机器人模型目录
    robot_model_dir: './data/robots/hab_spot_arm'

# ============================================================================
# 【部分6】导航任务配置
# ============================================================================
navigation:
  # ──────────────────────────────────────────────────────────────────────
  # 路径规划配置
  # ──────────────────────────────────────────────────────────────────────
  path_planning:
    # 生成的候选路径数量
    num_candidate_paths: 3
    # A*算法最大搜索深度
    max_search_depth: 100
    # 允许返回的最小路径长度
    min_path_length: 2

  # ──────────────────────────────────────────────────────────────────────
  # 导航循环配置
  # ──────────────────────────────────────────────────────────────────────
  navigation_loop:
    # 最大导航步数
    max_steps: 500
    # 到达目标的距离阈值 (米)
    goal_distance_threshold: 1.0
    # 碰撞检测的移动距离阈值 (米)
    collision_distance_threshold: 0.05
    # VLM调用的重试次数
    vlm_max_retries: 3
    # VLM调用之间的延迟 (秒)
    vlm_retry_delay: 1.0

# ============================================================================
# 【部分7】提示工程模板（VLM提示词）
# ============================================================================
prompts:
  # ──────────────────────────────────────────────────────────────────────
  # 系统级提示
  # ──────────────────────────────────────────────────────────────────────
  system:
    # 通用系统提示
    general: |
      你是一个专业的视觉导航系统。你将被提供：
      1. 当前环境的图像（第一人称视角）
      2. 导航目标和约束条件
      3. 可用的动作选项

      你的任务是：
      1. 分析视觉信息，识别环境中的关键特征
      2. 检测障碍物、通行区域和潜在危险
      3. 选择最优的下一步动作序列（4步）
      4. 解释你的决策理由

      始终优先考虑安全性，避免碰撞。

    # 导航特定系统提示
    navigation: |
      你是一个导航决策助手。

      【重要声明：这是 Habitat 虚拟仿真场景】
      - 你看到的前置RGB来自 Habitat-sim/habitat-lab 的虚拟环境渲染，不是真实世界照片。
      - 画面可能具有“合成/游戏感”：光照、材质、阴影可能与真实不同。

      你将看到图像输入（按顺序）：
      1) 前置RGB（时间 t；即“最后一次动作后的观测”）
      2) 前置Depth（时间 t；灰度可视化，近亮远暗）
      3) 当前楼层平面图（data/map/{floor}F.jpg）

      重要约束：
      - 不能假设“每一步进入路径的下一个房间”。当前位置/楼层必须基于视觉+平面图判断。
      - 只有当你非常确信到达目标房间时，才允许结束导航。

      可用动作：
      - move_forward: 向前移动0.25米
      - move_backward: 向后移动0.25米
      - turn_left: 向左转10度
      - turn_right: 向右转10度
      - stop: 停止（仅当 done=true 时允许）

      输出必须为严格 JSON，且满足一致性：
      - reached_goal 与 done 必须一致
      - done=true => actions=["stop"]
      - done=false => actions 长度=3 且不允许包含 stop

      JSON schema：
      {
        "done": true/false,
        "reached_goal": true/false,
        "current_room": "...",
        "current_floor": 1,
        "confidence": 0.0,
        "subgoal": "下一步必须到达的结构点（例如：穿过门洞D、进入楼梯间S、到达走廊拐角/路口）",
        "actions": ["move_forward", "turn_left", "..."],
        "reasoning": "一句话说明位置判断与动作意图"
      }

    # 更短的“仿真场景声明”（供 stage2 prompt 直接注入，避免把 navigation 整段重复注入）
    habitat_scene_declaration: |
      【场景声明（必须牢记）】
      - 这是 Habitat-sim/habitat-lab 的虚拟仿真环境渲染图像，不是真实场景的照片。
      - 因为是合成渲染：光照/材质/阴影可能不真实。

    # Stage2 核心导航策略（强调“连续推进+子目标保持+进度判断”，减少随机转向导致的不自然）
    stage2_navigation_policy: |
      【Stage2 核心策略（必须执行）】
      1) 子目标(subgoal)必须落到平面图结构点，并且必须是“下一步必须到达”的目标（例如根据平面图分析需要穿过门洞D、到达走廊拐角/路口、进入楼梯间并完成一跑楼梯到平台），并且需要分析注意点，例如要走楼梯时的注意事项。
      2) 控制判断：
         - 若连续碰撞：先 move_backward 脱困，再对齐通道方向，再 move_forward。
         -平衡左右两侧的空间，避免碰撞墙体等。
      4) 路线一致：优先按“整体导航指路”的大方向走，但当前位置必须基于视觉+平面图的空间变化判断，根据机器狗的在平面图中的位置和朝向提供导航建议，不能臆测已进入下一个房间。
      5) 楼梯规则：楼梯间内尽量连续 move_forward 完成一跑；只在平台/转折处少量转向；完成两跑并到达下一层落地/可进入走廊时才更新楼层。

    # 强约束：指导 VLM 如何合理使用记忆（关键记忆/经历记忆/整体指路）
    memory_usage_policy: |
      【记忆使用规则】
      你会看到三类“文字记忆”，它们都是辅助信息：
      1) 整体导航指路：全局路线，用于保持大方向一致。
      2) 最近经历/空间变化：你刚刚做过什么动作、是否碰撞、是否在楼梯间等。
      3) 长期关键记忆（LTM）：高置信定位/碰撞热点/上次subgoal等，提高空间理解能力。

      请注意：
      - 综合输出：需要综合所有可靠的资料输出需要执行的动作。
      - 空间理解：结合输入资料合理理解当前的空间解释，以帮助提高导航能力。

    anti_loop_policy: |
      【反转圈/反循环策略】
      你将看到 last_actions 与 episodic（最近经历）。如果出现以下模式，说明你可能陷入“原地转圈”循环：
      - 最近 8~12 步几乎都是 turn_left/turn_right（转向占比很高），但很少 move_forward/move_backward。

      脱困规则（按优先级）：
      1) 若没有碰撞提示：不要继续纯转向；请在 1~2 次小角度转向后，至少执行 1~2 次 move_forward 产生位移，获得视差。
      2) 若有碰撞/卡住提示：执行 move_backward 脱离，再转向对齐通道/楼梯方向，再 move_forward。
      3) 楼梯间内：尽量保持沿楼梯方向连续 move_forward；只在平台/转折处少量转向；不要左右来回震荡。

    # 平面图图例/语义（强烈建议在 Agent1 提示词中原样注入）
    # 说明：这是“文字 legend”，不改变 VLM 的三图输入结构。
    floorplan_legend: |
      【平面图图例 / 符号语义（请严格遵守）】
      - Start01：户外起点位置（Outdoor / Outside）。
      - S：楼梯间（Staircase / Stairwell）。
      - H：走廊（Hallway / Corridor）。
      - R：房间（Room）。
      - D：洞口/门洞/开口（Doorway / Opening），表示可以穿过的通道位置。
      - W：窗户（Window），通常不可穿过；靠近时可能看到窗框/玻璃。
      - C：柱子（Column），不可穿过；会形成遮挡/窄通道。

      【编号规则（用于判断楼层）】
      - 标签后的数字用于编号；其中“第一个数字”表示楼层位置/楼层编号。
      - 例如：H201/R230/S206 的第一个数字为 2，表示 2F；H102 的第一个数字为 1，表示 1F。

      【使用方式】
      - 在平面图上优先识别你附近的 H/S/R/Start 标签与 D/W/C。
      - 通过空间移动和平面图判断进入的房间/走廊编号。
      - 楼层通过楼梯间的上下行走来确定。

    # 先进提示：平面图驱动的“协议化”决策流程（简短、强约束、可执行）
    floorplan_navigation_protocol: |
      【平面图驱动导航协议（请按顺序执行）】
      Step A — Map-First 定位：
      - 先在平面图上找你“可能所处区域”的拓扑：H(走廊)；R(房间)；S(楼梯间)；D(门洞)。
      - 用编号规则先锁定楼层：优先从你在图上能读到的 H/S/R 标签推断楼层。

      Step B — Vision 观察：
      - 观察当前前置RGB以及Depth图像（灰度可视化，近亮远暗）：判断面前是否可通行、距离/障碍、门洞/走廊方向。

      Step C — Subgoal 选择（必须落到地图上的结构点）：
      - 从当前位置假设出发，结合当前前视图，思考确定 subgoal（例如 门洞/走廊拐角/楼梯平台）。

      Step D — 3步微动作规划：
      - 合理决定接下来的动作方向。

      Step E — 自检（输出前必须检查）：
      - 动作序列是否与 subgoal 对应？
      - 动作执行是否有利于接近 subgoal？

    confidence_calibration: |
      【confidence 标定（0~1）】
      - 0.85~1.0：你能同时在平面图与视觉中对齐到明确的空间位置。
      - 0.55~0.85：大概率在某走廊/区域。
      - 0.30~0.55：只知道大概楼层或大概在走廊/房间类型。
      - 0.00~0.30：几乎无法定位。

    navigation_output_template: |
      【reasoning 字段模板（单行字符串，建议用“|”分隔）】
      loc=...; floor=...; map_evidence=...; vision_delta=...; subgoal=...; plan=...; risk=...

    stair_traversal_policy: |
      【楼梯/跨楼层规则（必须严格遵守）】
      - 机器狗可以直接走楼梯。
      - 楼层的确定：只有“走完一个完整楼梯间”后才会升高一个楼层。
        - 一个完整楼梯间 = 两跑楼梯（two flights）。
      - 在楼梯间(Sxx)内部移动时：
        - 不要因为看到楼梯或墙体变化就提前把 current_floor 改成下一层。
        - 只在你完成两跑并到达下一层的“落地/平台并可进入走廊/门洞(D)”时，才更新 current_floor。
      - 楼梯阶段动作策略（5步微规划）：
        - 一旦决定“上/下楼”，优先保持同向前进（多步 move_forward）完成一跑；仅在平台/转折处用少量 turn 调整方向。
        - 若连续碰撞/卡住：先转向重新对齐楼梯方向，再继续前进；不要在楼梯上来回左右震荡。
      - 平面图使用：
        - Sxx 是楼梯间节点；跨楼层时你应在 Sxx 内完成两跑，再到达下一层的 Syy/Hyy。

    # 阶段1：路径选择后生成“整体指路描述”的要求（由 Agent1.select_best_path 使用）
    route_guidance_requirements: |
      【阶段1：整体导航指路（必须生成）】
      - 你在选择最佳路径后，需要生成一段“整体导航过程描述”（像人类指路一样，尽量详细）。
      - 指路必须基于“选中的路径节点序列”，并明确：
        1) 从起点到目标会依次经过哪些 H/R/S/Start 节点（用“→”串起来）。
        2) 哪些段是在走廊/房间内推进，哪些段进入楼梯间 Sxx。
        3) 跨楼层规则：只有完成一个完整楼梯间（两跑楼梯）并到达下一层落地/可进入走廊或门洞D时，才算楼层改变。
      - 语言风格：像“从A出来，沿走廊前进到…，在楼梯间…完成两跑上楼，出楼梯进入H2xx…”这样。
      - 输出字段 route_description 必须是中文、多行文本，允许换行。

      【输出 JSON】
      {
        "choice": 1,
        "route_description": "..."
      }

  # ──────────────────────────────────────────────────────────────────────
  # 用户提示模板
  # ──────────────────────────────────────────────────────────────────────
  user:
    # 路径选择提示
    path_selection: |
      请分析这三条候选路径的楼层平面图，选择最高效的路径到达目标 {target}。

      候选路径：
      {paths}

      请选择最佳路径并解释理由。

    # 动作决策提示
    action_generation: |
      当前位置提示：{current_room}
      目标房间：{target_room}
      规划路径（参考）：{planned_path}

      请基于“最后一次动作后的前置视图”（单帧RGB，另可选Depth）和当前楼层平面图：
      1) 判断你所在的房间与楼层（输出 current_room/current_floor/confidence）
      2) 输出下一步必须到达的结构性子目标 subgoal（例如穿过门洞D、进入楼梯间S、到达走廊拐角/路口、到达楼梯平台）
      2) 若未到达目标：输出接下来 5 步动作（不得包含 stop）
      3) 若已到达目标：输出 done=true 且 actions=["stop"]

      只输出严格 JSON。

# ============================================================================
# 说明和注意事项
# ============================================================================
#
# 1. 参数参考：
#    - 所有传感器参数参考 tests/test_habitat_interactive_enhanced.py
#    - Agent高度默认 0.55m，碰撞半径 0.28m
#    - 前置摄像头：720×1280分辨率，水平视场角90度
#    - 后置摄像头：480×640分辨率，俯视角度
#
# 2. 图像处理：
#    - image_processing.enabled 建议保持 true（两帧RGB+平面图输入，需控制上传与延迟）
#    - 前置RGB会做适度缩放与JPEG压缩以提升吞吐
#    - 平面图会保留较高质量以保证结构/文字清晰
#
# 3. 可视化：
#    - 可通过 system.visualization.enabled 启用/禁用可视化
#    - 也可通过命令行参数 --enable-viz 覆盖此设置
#
# 4. 配置加载：
#    - 项目使用 UnifiedConfigLoader 加载此配置文件
#    - 所有代码都应该通过统一的配置加载器访问参数
#    - 不要硬编码参数值
#
# ============================================================================
